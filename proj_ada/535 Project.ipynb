{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e4ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "from learners import weak, strong \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceaf2527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing - changing to one-hot encoding \n",
    "#it is assumed that catagorical datasets have the first entry as the label, can easily change index for any given data \n",
    "df = pd.read_csv(\"agaricus-lepiota.data\", header=None).drop(columns=11) #only column with missing data \n",
    "vals_col = {}\n",
    "pro_data = []\n",
    "\n",
    "for c in df.columns:\n",
    "    uvals = df[c].unique()\n",
    "    l = uvals.tolist()\n",
    "    vals_col[c] = l\n",
    "    \n",
    "for r in df.iterrows(): \n",
    "    r = r[1].to_numpy()\n",
    "    r = np.insert(r,11, 0) # circumvent indexing problems \n",
    "    encoded = []\n",
    "    for i,feature in enumerate(r):\n",
    "        if i != 11: #skip column\n",
    "            if i == 0: #must have features either -1, 1 for loss function to work \n",
    "                if feature == 'p': \n",
    "                    encoded.append(-1.0)\n",
    "                else: \n",
    "                    encoded.append(1.0)\n",
    "            else: \n",
    "                one_hot = np.zeros(len(vals_col[i])) \n",
    "                one_hot[vals_col[i].index(feature)] = 1.0\n",
    "                encoded.append(one_hot.tolist())\n",
    "    encoded.insert(0, 1/8124) #set equal weights\n",
    "    pro_data.append(encoded)\n",
    "train = pro_data[:6092] #75, 25 test train split \n",
    "test = pro_data[6093:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abe63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting learners.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile learners.py\n",
    "class strong: \n",
    "    def __init__(self): \n",
    "        self.ensemble = [] \n",
    "        \n",
    "    def pred(self, dp): \n",
    "        output = 0\n",
    "        for i in self.ensemble: \n",
    "            output += i.partition(dp)*i.weight\n",
    "        if output > 0: \n",
    "            return 1.0\n",
    "        if output < 0: \n",
    "            return -1.0 \n",
    "        \n",
    "class weak: \n",
    "    def __init__(self, mark, direct): \n",
    "        self.marker = mark\n",
    "        self.weight = 0  \n",
    "        self.dir = direct\n",
    "        \n",
    "    def partition(self, dp): \n",
    "        x = self.marker[0]\n",
    "        y = self.marker[1]\n",
    "        val = dp[x][y]\n",
    "        if self.dir == 'pos': \n",
    "            return 1.0 if val == 1.0 else -1.0\n",
    "        if self.dir == 'neg': \n",
    "            return -1.0 if val == 1.0 else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2a6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exponential Loss function/minimizer \n",
    "def exp_loss(data): \n",
    "    losses = []\n",
    "    for k,feature  in enumerate(data[0][2:]):  #iterate over every feature \n",
    "        for coord in range(len(feature)):   #iterate over all possible vals of feature\n",
    "            preds_neg = []\n",
    "            preds_pos = []\n",
    "            neg_err = 0\n",
    "            pos_err = 0 \n",
    "            for dp in data: #for each of these, make prediction on every datapoint, considering both dir of decision\n",
    "                mark = [k+2, coord] #accounting for fact that first two entries are weight, label\n",
    "                w_pos = weak(mark, 'pos' )\n",
    "                w_neg = weak(mark, 'neg')\n",
    "                preds_pos.append(w_pos.partition(dp))\n",
    "                preds_neg.append(w_neg.partition(dp))\n",
    "            for i in range(len(data)):    #compare with actual labels, calculating error based on incorrectness                    \n",
    "                if preds_pos[i] != data[i][1]:\n",
    "                    pos_err += data[i][0]  #add weights \n",
    "                if preds_neg[i] != data[i][1]: \n",
    "                    neg_err += data[i][0]\n",
    "            losses.append([pos_err, mark, 'pos']) #store all losses along with identifier of which coord was used to classify\n",
    "            losses.append([neg_err, mark, 'neg'])\n",
    "    min_err = None       #find smallest error \n",
    "    for err in losses: \n",
    "        if min_err == None: \n",
    "            min_err = err \n",
    "        else: \n",
    "            if err[0] < min_err[0]:\n",
    "                if err[0] == 0.0: # I have no clue how this is even possible \n",
    "                    #print('uh oh')\n",
    "                    continue \n",
    "                else: \n",
    "                    min_err = err \n",
    "                         #calculate weight of new weak learner, then instantiate and add to ensemble \n",
    "    alph = .5 * math.log((1-min_err[0])/min_err[0])\n",
    "    new = weak(min_err[1], min_err[2])\n",
    "    new.weight = alph \n",
    "                        #for updating data point weight\n",
    "    return data, new   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ae2e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight updater \n",
    "def data_weight(data, new): \n",
    "    nw_raw = 0 \n",
    "    for dp in data: #update \n",
    "        dp[0] = dp[0] * math.exp(-dp[1] * new.weight * new.partition(dp))\n",
    "        nw_raw += dp[0]\n",
    "    for dp in data: #normalize\n",
    "        dp[0] = dp[0] / nw_raw\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21695d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf(sl, test): \n",
    "    mat = np.array([[0,0],  #rows denote true values(top=1.0), cols denote predicted(LHS=1.0)\n",
    "                  [0,0]])   #(1) - edible, (-1) - inedible \n",
    "    true_pos = 0 \n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0 \n",
    "    for dp in test:\n",
    "        label = dp[1]\n",
    "        pred = sl.pred(dp)\n",
    "        if label == 1.0:\n",
    "            if pred == 1.0:\n",
    "                mat[0, 0] += 1  # True positive\n",
    "            else:\n",
    "                mat[0, 1] += 1  # False negative\n",
    "        if label == -1.0:\n",
    "            if pred == -1.0: \n",
    "                mat[1, 1] += 1  # True negative\n",
    "            else: \n",
    "                mat[1, 0] += 1  # False positive\n",
    "    return mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ee06539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(tr, te):\n",
    "    SL = strong()\n",
    "    iterr = 0 \n",
    "    best_SL = strong()\n",
    "    best_acc = 0 \n",
    "    while iterr <= 100: \n",
    "        if iterr == 0:  \n",
    "            data,new_learner = exp_loss(tr)\n",
    "        else: \n",
    "            data,new_learner = exp_loss(weighted_data)\n",
    "        SL.ensemble.append(new_learner)\n",
    "        weighted_data = data_weight(data, new_learner)\n",
    "        if iterr%5 == 0: \n",
    "            c = conf(SL, te)\n",
    "            print(f'Confusion matrix for {iterr} weak learners in the ensemble: \\n{c}\\n\\n')\n",
    "            acc = c[0,0] + c[1,1]\n",
    "            if acc> best_acc: \n",
    "                best_acc = acc\n",
    "                best_SL.ensemble = SL.ensemble.copy()\n",
    "        iterr += 1 \n",
    "    print(f'The most accurate Adaboost with decision stumps model is that with {len(best_SL.ensemble)} weak learners \\n having an accuracy of {best_acc/8124}%' )\n",
    "    return best_SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3a4b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for 0 weak learners in the ensemble: \n",
      "[[ 525    0]\n",
      " [1006  500]]\n",
      "\n",
      "\n",
      "Confusion matrix for 5 weak learners in the ensemble: \n",
      "[[ 525    0]\n",
      " [  44 1462]]\n",
      "\n",
      "\n",
      "Confusion matrix for 10 weak learners in the ensemble: \n",
      "[[ 525    0]\n",
      " [   8 1498]]\n",
      "\n",
      "\n",
      "Confusion matrix for 15 weak learners in the ensemble: \n",
      "[[ 525    0]\n",
      " [   8 1498]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_test(train, test)\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mtrain_test\u001b[1;34m(tr, te)\u001b[0m\n\u001b[0;32m      8\u001b[0m     data,new_learner \u001b[38;5;241m=\u001b[39m exp_loss(tr)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m---> 10\u001b[0m     data,new_learner \u001b[38;5;241m=\u001b[39m exp_loss(weighted_data)\n\u001b[0;32m     11\u001b[0m SL\u001b[38;5;241m.\u001b[39mensemble\u001b[38;5;241m.\u001b[39mappend(new_learner)\n\u001b[0;32m     12\u001b[0m weighted_data \u001b[38;5;241m=\u001b[39m data_weight(data, new_learner)\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mexp_loss\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dp \u001b[38;5;129;01min\u001b[39;00m data: \u001b[38;5;66;03m#for each of these, make prediction on every datapoint, considering both dir of decision\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     mark \u001b[38;5;241m=\u001b[39m [k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m, coord] \u001b[38;5;66;03m#accounting for fact that first two entries are weight, label\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     w_pos \u001b[38;5;241m=\u001b[39m weak(mark, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[0;32m     13\u001b[0m     w_neg \u001b[38;5;241m=\u001b[39m weak(mark, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m     preds_pos\u001b[38;5;241m.\u001b[39mappend(w_pos\u001b[38;5;241m.\u001b[39mpartition(dp))\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\535\\Project\\learners.py:15\u001b[0m, in \u001b[0;36mweak.__init__\u001b[1;34m(self, mark, direct)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mweak\u001b[39;00m: \n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mark, direct): \n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarker \u001b[38;5;241m=\u001b[39m mark\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
